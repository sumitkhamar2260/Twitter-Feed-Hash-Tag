{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter Scraping.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DV17Dkdl5cu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "outputId": "1d46065a-7ed8-4bfd-83dd-8f302df7467f"
      },
      "source": [
        "!pip install preprocessor\n",
        "from tweepy import OAuthHandler\n",
        "from tweepy.streaming import StreamListener\n",
        "import tweepy\n",
        "import json\n",
        "import pandas as pd\n",
        "import csv\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "import string\n",
        "import preprocessor as p\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting preprocessor\n",
            "  Downloading https://files.pythonhosted.org/packages/96/ad/d9f4ffb9bb97d1cb5bcb876b7932571d4dbaa3eff1701ad45d367f0ea27b/preprocessor-1.1.3.tar.gz\n",
            "Building wheels for collected packages: preprocessor\n",
            "  Building wheel for preprocessor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for preprocessor: filename=preprocessor-1.1.3-cp36-none-any.whl size=4478 sha256=df65d92e8a047252ee0778f2816ee5543f7f1cbbc75b5009d14fb83950059210\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/c1/a2/21fbcfd80d76576bbf148991a66f00730f541f265c7600000f\n",
            "Successfully built preprocessor\n",
            "Installing collected packages: preprocessor\n",
            "Successfully installed preprocessor-1.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7SEE-1YmACg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "consumer_key = 'tatmDuCe7KV9JHNG2qXQrJcF5'\n",
        "consumer_secret ='SJso5hXiasykCHQvw6tIY3GYUz8KTr3SI6evbmOQoAcpF9kKmM'\n",
        "access_key = '877899383353872390-SVxBOFTQxzeN7QGF8NtoOZIz2FCtIj2'\n",
        "access_secret = 'f3TvagQBcamFYxynPZcmfBMATdADjfl7LL3GZyqgR1tq1'\n",
        "# Pass your twitter credentials to tweepy via its OAuthHandler\n",
        "auth = OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_key, access_secret)\n",
        "api = tweepy.API(auth)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKEHwbihmDVd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def scraptweets(search_words, date_since, numTweets, numRuns):\n",
        "\n",
        "    db_tweets = pd.DataFrame(columns = ['username', 'acctdesc', 'location', 'following',\n",
        "                                        'followers', 'totaltweets', 'usercreatedts', 'tweetcreatedts',\n",
        "                                        'retweetcount', 'text', 'hashtags'])\n",
        "    for i in range(0, numRuns):\n",
        "        start_run = time.time()\n",
        "        \n",
        "        # Collect tweets using the Cursor object\n",
        "        # .Cursor() returns an object that you can iterate or loop over to access the data collected.\n",
        "        # Each item in the iterator has various attributes that you can access to get information about each tweet\n",
        "        tweets = tweepy.Cursor(api.search, q=search_words, lang=\"en\", since=date_since, tweet_mode='extended').items(numTweets)\n",
        "\n",
        "        # Store these tweets into a python list\n",
        "        tweet_list = [tweet for tweet in tweets]\n",
        "\n",
        "        # Obtain the following info (methods to call them out):\n",
        "            # user.screen_name - twitter handle\n",
        "            # user.description - description of account\n",
        "            # user.location - where is he tweeting from\n",
        "            # user.friends_count - no. of other users that user is following (following)\n",
        "            # user.followers_count - no. of other users who are following this user (followers)\n",
        "            # user.statuses_count - total tweets by user\n",
        "            # user.created_at - when the user account was created\n",
        "            # created_at - when the tweet was created\n",
        "            # retweet_count - no. of retweets\n",
        "            # (deprecated) user.favourites_count - probably total no. of tweets that is favourited by user\n",
        "            # retweeted_status.full_text - full text of the tweet\n",
        "            # tweet.entities['hashtags'] - hashtags in the tweet\n",
        "\n",
        "        # Begin scraping the tweets individually:\n",
        "        noTweets = 0\n",
        "\n",
        "        for tweet in tweet_list:\n",
        "\n",
        "            # Pull the values\n",
        "            username = tweet.user.screen_name\n",
        "            acctdesc = tweet.user.description\n",
        "            location = tweet.user.location\n",
        "            following = tweet.user.friends_count\n",
        "            followers = tweet.user.followers_count\n",
        "            totaltweets = tweet.user.statuses_count\n",
        "            usercreatedts = tweet.user.created_at\n",
        "            tweetcreatedts = tweet.created_at\n",
        "            retweetcount = tweet.retweet_count\n",
        "            hashtags = tweet.entities['hashtags']\n",
        "\n",
        "            try:\n",
        "                text = tweet.retweeted_status.full_text\n",
        "            except AttributeError:  # Not a Retweet\n",
        "                text = tweet.full_text\n",
        "\n",
        "            # Add the 11 variables to the empty list - ith_tweet:\n",
        "            ith_tweet = [username, acctdesc, location, following, followers, totaltweets,\n",
        "                         usercreatedts, tweetcreatedts, retweetcount, text, hashtags]\n",
        "\n",
        "            # Append to dataframe - db_tweets\n",
        "            db_tweets.loc[len(db_tweets)] = ith_tweet\n",
        "\n",
        "            # increase counter - noTweets  \n",
        "            noTweets += 1\n",
        "        \n",
        "        # Run ended:\n",
        "        end_run = time.time()\n",
        "        duration_run = round(end_run-start_run, 2)\n",
        "        \n",
        "        print('no. of tweets scraped for run {} is {}'.format(i, noTweets))\n",
        "        print('time take for {} run to complete is {}'.format(i, duration_run))\n",
        "        \n",
        "        #time.sleep(900) #15 minute sleep time\n",
        "\n",
        "        \n",
        "    # Once all runs have completed, save them to a single csv file:    \n",
        "    # Obtain timestamp in a readable format:\n",
        "    from datetime import datetime\n",
        "    to_csv_timestamp = datetime.today().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "    # Define working path and filename\n",
        "    #path = os.getcwd()\n",
        "    filename ='/content/drive/My Drive/Colab Notebooks/twitter.csv'\n",
        "\n",
        "    # Store dataframe in csv with creation date timestamp\n",
        "    db_tweets.to_csv(filename, index = False)\n",
        "    \n",
        "    print('Scraping has completed!')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Sb6lc-cmmUt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "76d140ed-1f27-4f3f-dcc4-c4dddfbb0b54"
      },
      "source": [
        "search_words = \"#Charlottesville\"\n",
        "date_since = \"2019-10-10\"\n",
        "numTweets = 2000\n",
        "numRuns = 1\n",
        "# Call the function scraptweets\n",
        "scraptweets(search_words, date_since, numTweets, numRuns)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no. of tweets scraped for run 0 is 926\n",
            "time take for 0 run to complete is 15.76\n",
            "Scraping has completed!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
